{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# don't ever remove\n",
    "%env QT_QPA_PLATFORM=wayland"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import gym_snake\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "from gym_snake.envs.constants import Direction4, Action4"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib tk"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = gym.make(\"Snake-8x8-v0\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.ion()\n",
    "plt.show()\n",
    "\n",
    "def render(obs):\n",
    "    plt.imshow(obs)\n",
    "    plt.draw()\n",
    "    plt.pause(0.001)\n",
    "render(obs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    #env.render\n",
    "    render(obs)\n",
    "    action = random.choice([0, 1, 2])\n",
    "\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    print(info)\n",
    "\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "\n",
    "        for y in range(obs.shape[1]):\n",
    "            for x in range(obs.shape[0]):\n",
    "                pixel = obs[y][x]\n",
    "                if pixel[2] == 255:\n",
    "                    hlava = [y, x]\n",
    "                if pixel[1] == 255:\n",
    "                    telo = [y, x]\n",
    "\n",
    "        if hlava[0] == telo[0]:#stejny radek\n",
    "            if hlava[1] > telo[1]:\n",
    "                look_dir = Direction4.east\n",
    "            else:\n",
    "                look_dir = Direction4.west\n",
    "        else:\n",
    "            if hlava[0] > telo[0]:\n",
    "                look_dir = Direction4.south\n",
    "            else:\n",
    "                look_dir = Direction4.north\n",
    "\n",
    "\n",
    "\n",
    "        info =\n",
    "\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Inspired by Tutorial: https://www.youtube.com/watch?v=dpBKz1wxE_c\n",
    "# Using Deep Q neural Network on CartPole environment\n",
    "# Edited\n",
    "# by Patrik VÃ¡cal\n",
    "\n",
    "from collections import deque\n",
    "from time import sleep\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.optimizer_v2.adam import Adam"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# To fix tf2 speed - https://github.com/tensorflow/tensorflow/issues/33024\n",
    "tf.compat.v1.disable_eager_execution()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Prepare algorithm ###\n",
    "# Q Network - we use one layer so it is not deep\n",
    "class DQNAgent:\n",
    "    def __init__(self, env, gamma=0.97, epsilon=1.0, epsilon_decay=0.98, min_epsilon=0.01, learning_rate=0.01,\n",
    "                 state_dim=None, action_size=None):\n",
    "        self.gamma = gamma  # discount rate\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.state_dim = state_dim or env.observation_space.shape\n",
    "        self.action_size = action_size or env.action_space.n\n",
    "\n",
    "        self.replay_buffer = deque(maxlen=2000)\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential([\n",
    "            Dense(200, input_shape=self.state_dim, activation=\"relu\"),\n",
    "            Dense(100, activation=\"relu\"),\n",
    "            Dense(5, activation=\"relu\"),\n",
    "            Dense(self.action_size, activation=None)\n",
    "        ])\n",
    "        model.compile(loss=\"mean_squared_error\", optimizer=Adam(lr=self.learning_rate))\n",
    "        model.summary(line_length=120)\n",
    "        return model\n",
    "\n",
    "    def get_action(self, state, skip_random=False):\n",
    "        if not skip_random and random.random() < self.epsilon:\n",
    "            action_random = random.choice(range(self.action_size))  # Get random action\n",
    "            return action_random\n",
    "\n",
    "        else:\n",
    "            q_state = self.model.predict(np.array([state]))  # Select Q values for current state from Q table\n",
    "            action_greedy = np.argmax(q_state)               # Select action with highest Q value\n",
    "            return action_greedy\n",
    "\n",
    "    def add_experience(self, experience):\n",
    "        self.replay_buffer.append(experience)\n",
    "\n",
    "    def train_network(self, batch_size=50):\n",
    "        # Do not train if not enough experience\n",
    "        if len(self.replay_buffer) < batch_size:\n",
    "            return\n",
    "\n",
    "        samples = random.choices(self.replay_buffer, k=batch_size)\n",
    "        state, action, next_state, reward, done = (list(col) for col in zip(*samples))  # Make list of states, list of actions, ...\n",
    "\n",
    "        q_current = self.model.predict(np.array(state))            # Get Q values for all possible current actions\n",
    "\n",
    "        q_next = self.model.predict(np.array(next_state))          # Get Q values for all possible next actions\n",
    "        q_next[done] = 0                                                   # Set all future(next) Q values to 0 if this step is the last one\n",
    "        q_target = reward + (self.gamma * np.max(q_next, axis=1))  # Calculate target Q = reward + max next Q value\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            q_update = q_target[i] - q_current[i][action[i]]        # = how much to move Q value in table from current value\n",
    "            q_current[i][action[i]] += self.learning_rate * q_update   # Update Q value a little based on LEARNING_RATE\n",
    "\n",
    "        # Train the neural network with our updated Q value\n",
    "        self.model.fit(np.array(state), np.array(q_current), verbose=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Train ###\n",
    "agent = DQNAgent(env, learning_rate=0.001)\n",
    "\n",
    "# Was stable after about 100 episodes\n",
    "EPISODES = 200\n",
    "for episode in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    cumulative_reward = 0\n",
    "    i = 0\n",
    "    env_i = 0\n",
    "    while not done:\n",
    "        action = agent.get_action(state)                                 # Action = get action from Agent\n",
    "        next_state, reward, done, info = env.step(action)                # Do the Action\n",
    "        cumulative_reward += reward\n",
    "\n",
    "        if done and env.reset():\n",
    "            reward = -100  # Punish for crashing\n",
    "\n",
    "        agent.add_experience([state, action, next_state, reward, done])  # Add experience to replay buffer\n",
    "        agent.train_network()                                            # Train the network\n",
    "\n",
    "        i += 1\n",
    "        state = next_state                                               # Set State to next State\n",
    "\n",
    "        # env.render()\n",
    "        # sleep(0.05)\n",
    "\n",
    "    # Decay epsilon each episode\n",
    "    agent.epsilon = max(agent.min_epsilon, agent.epsilon * agent.epsilon_decay)\n",
    "\n",
    "    print(f\"e: {episode:03}, eps: {agent.epsilon:.2f}, r: {cumulative_reward}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "### Test run ###\n",
    "print()\n",
    "while True:\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.get_action(state, skip_random=True)              # Action = get action from Agent\n",
    "        state, reward, done, info = env.step(action)  # Do the Action\n",
    "        env.render()\n",
    "        sleep(0.5)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}